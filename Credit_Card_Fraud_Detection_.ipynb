{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l-1ceFh-b5O"
      },
      "source": [
        "<h1 style=\"text-align: center;\">Credit Card Fraud Detection using Machine Learning</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pdBLSyRlLST"
      },
      "source": [
        "<h2 style=\"text-align: center;\">Import Neccessary Libraries</h2>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk5h4JMjkxT6"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPWJOEq0lGPq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTZr55CjlVyA"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USoAFTkAloPk"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rPR_-wflsCA"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRsGEc34lw3R"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d mlg-ulb/creditcardfraud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0217PBFfmLWL"
      },
      "outputs": [],
      "source": [
        "! unzip /content/creditcardfraud.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyGLUZOPmeJi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest, StackingClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
        "precision_recall_curve, average_precision_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a requirements.txt file\n",
        "! pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "_83LhJrwJ_GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC__wPHAp4Qx"
      },
      "source": [
        "<h2 style=\"text-align: center;\">Data Preprocessing</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIJiOl1NRhu-"
      },
      "source": [
        "#### Steps to prepare the credit card fraud dataset:\n",
        "##### 1. Load the creditcard.csv dataset into a DataFrame.\n",
        "##### 2. Check for and handle any missing/null values.\n",
        "##### 3. Identify and remove duplicate records to avoid data leakage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD7_U4zqmvUg"
      },
      "outputs": [],
      "source": [
        "# Read creditcard.csv into a pandas dataframe\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-n5IvgT0SAd"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYSDEHbtoVYO"
      },
      "outputs": [],
      "source": [
        "# Print the dataframe's information\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6XbJR2spK2q"
      },
      "outputs": [],
      "source": [
        "# Print the amount of null values in the dataframe\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_TCOo_F0f89"
      },
      "outputs": [],
      "source": [
        "# Check for duplicated values in the dataframe\n",
        "df.duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtDSESp-1EIy"
      },
      "outputs": [],
      "source": [
        "# Print the sum of the duplicated values\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZh8pAw70wFw"
      },
      "outputs": [],
      "source": [
        "# Removes the duplicated values in the dataframe\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsw9H78Z06p9"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAbZcsSgUBv2"
      },
      "source": [
        "<h2 style=\"text-align: center;\">Data Visualization</h2>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the following distributions:\n",
        "##### 1. Normal vs fraudulent transactions to visualize the class imbalance in the dataset\n",
        "##### 2. Fraud transactions over time (hours elapsed since the first transaction) to show when fraudulent activity is most frequent\n",
        "##### 3. Transaction amount by class to show the difference in the amount range for fraud vs normal transactions"
      ],
      "metadata": {
        "id": "rOrvFU8EG7Os"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcrWsTem9nX0"
      },
      "outputs": [],
      "source": [
        "# Print the unique values in class column\n",
        "df['Class'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SBi6qq_08wW"
      },
      "outputs": [],
      "source": [
        "# Print the value count for the class column\n",
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Time (seconds) to hours\n",
        "df['Hours'] = df['Time'].apply(lambda x: x/ 3600)"
      ],
      "metadata": {
        "id": "L0K89lz4nn4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of normal vs fraudulent transactions\n",
        "# Add value labels to show the exact count per class\n",
        "\n",
        "plt.figure(figsize = (6, 4))\n",
        "normal_vs_fraud_plot = sns.countplot(x = 'Class', data = df)\n",
        "\n",
        "# Add the count labels aboves bar\n",
        "for patch in normal_vs_fraud_plot.patches:\n",
        "    height = patch.get_height()\n",
        "    if height > 0:\n",
        "        plt.text(patch.get_x() + patch.get_width() / 2, height + 0.01, f'{int(height):,}', ha = 'center', va = 'bottom', fontsize = 9)\n",
        "\n",
        "plt.title('Distribution of Normal vs Fraudulent Transactions')\n",
        "plt.xticks([0, 1], ['Normal', 'Fraudulent'])\n",
        "plt.xlabel('Transaction Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NlCaqVMg3B0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of fradulent transactions over time\n",
        "# Convert Time (seconds) to hours elapsed since the first transaction\n",
        "# Filter fraud transactions and plot distribution over time\n",
        "# Add value labels to show the exact amount of fraud cases in each hour\n",
        "\n",
        "# Filter fraud transactions\n",
        "fraud_df = df[df['Class'] == 1]\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize = (16, 6))\n",
        "fraud_over_time_plot = sns.histplot(fraud_df, x = 'Hours', bins = 48, kde = True)\n",
        "\n",
        "# Add count labels above bars\n",
        "for patch in fraud_over_time_plot.patches:\n",
        "    height = patch.get_height()\n",
        "    if height > 0:\n",
        "        plt.text(patch.get_x() + patch.get_width() / 2, height + 0.01, int(height), ha ='center', fontsize = 9)\n",
        "\n",
        "plt.title('Distribution of Fraudulent Transactions over Time')\n",
        "plt.xlabel('Time Elapsed since the First Transaction (Hours)')\n",
        "plt.ylabel('Number of Fraudulent Transactions')\n",
        "plt.xticks(range(0, 49))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t7Ho0VYO-HFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of transaction amount by class\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "transactions_by_amount_plot = sns.violinplot(x = 'Class', y = 'Amount', data = df, cut = 0, scale = 'width', inner = 'box')\n",
        "plt.title('Distribution of Transactions Amount by Class')\n",
        "plt.xticks([0, 1], ['Normal', 'Fraudulent'])\n",
        "plt.xlabel('Transaction Class')\n",
        "plt.ylabel('Transaction Amount')\n",
        "plt.grid(axis ='y', linestyle= '--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "koRSk8aRI_gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all the data visualization plots to drive\n",
        "save_dir = '/content/drive/MyDrive/Credit Card Fraud Detection/assets'\n",
        "os.makedirs(save_dir, exist_ok = True)\n",
        "\n",
        "figures_to_save = {\n",
        "'normal_vs_fraud_countplot.png': normal_vs_fraud_plot,\n",
        "'fraud_over_time_histplot .png': fraud_over_time_plot,\n",
        "'transactions_by_amount_violinplot.png': transactions_by_amount_plot,\n",
        "}\n",
        "\n",
        "for filename, ax in figures_to_save.items():\n",
        "    filepath = os.path.join(save_dir, filename)\n",
        "    ax.figure.savefig(filepath, dpi = 300, bbox_inches = 'tight')\n",
        "    print(f'Saved {filepath}')"
      ],
      "metadata": {
        "id": "wEEYzE7kLP-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"text-align: center;\">Model Development</h2>"
      ],
      "metadata": {
        "id": "DYe0aEHLIPL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The following was done to train, test and evaluate the models\n",
        "##### 1. Normalize the Time and Amount features using StandardScaler to ensure they are on the same scale as the other PCA-transformed features.\n",
        "##### 2. Separate features (x) and target variable (y), then split into training and testing sets\n",
        "##### 3. Apply sampling techniques to handle class imbalance:\n",
        "#####       - Random Under Sampling (RUS): This method randomly remove samples from the majority class to balance the class distribution\n",
        "#####       - Random Over Sampling (ROS): This method randomly duplicating samples from the minority class to balance the class distribution\n",
        "#####       - Synthetic Minority Oversampling Technique (SMOTE): This method generates synthetic samples for the minority class to balance the class imbalance\n",
        "##### 4. Train the following models on each sampled dataset:\n",
        "#####       - Logistic Regression (LR)\n",
        "#####       - Decision Tree (DT)\n",
        "#####       - Random Forest (RF)\n",
        "#####       - XGBoost (XGB)\n",
        "#####       - Stacking Ensemble Model: Combines Random Forest and XGBoost with Logistic Regression as meta-learner\n",
        "#####       - Multi-layer Perceptron (MLP)\n",
        "#####       - Isolation Forest (unsupervised anomaly detection)\n",
        "##### 5. Evaluate model performance using:\n",
        "#####       - Classification report (precision, recall, F1-score)\n",
        "#####       - Confusion matrix\n",
        "#####       - ROC curve and AUC score\n",
        "#####       - Precision-Recall curve\n",
        "#####       - Average precision score\n",
        "\n",
        "\n",
        "##### Note: Overfitting was observed in all models trained on ROS and SMOTE apart from Isolation Forest (Unsupervised) and Logistic Regression (supervised), while RUS produced more balanced performance."
      ],
      "metadata": {
        "id": "xr6IAkmS5-fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values in the Amount, Time and Hours Column column using standardscaler\n",
        "\n",
        "df[['Amount', 'Time']] = StandardScaler().fit_transform(df[['Amount', 'Time']])"
      ],
      "metadata": {
        "id": "_KCvHUtAaqe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt the features (independent variables) and target variable (dependent variable)\n",
        "\n",
        "x = df.drop(['Class', 'Hours'], axis = 1)\n",
        "y = df['Class']"
      ],
      "metadata": {
        "id": "MTyGNTjIbG_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vxiY-vmqtOL"
      },
      "source": [
        "<h3 style=\"text-align: center;\">Random Under Sampling</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oioDdgUEtF6Z"
      },
      "outputs": [],
      "source": [
        "# Initialize RandomUnderSampler\n",
        "\n",
        "RUS = RandomUnderSampler(random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EQ1iFRkqLQq"
      },
      "outputs": [],
      "source": [
        "# Apply RandomUnderSampler\n",
        "\n",
        "x_under_sampled, y_under_sampled = RUS.fit_resample(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF9odd_xrJMz"
      },
      "outputs": [],
      "source": [
        "# Convert the resampled arrays back to a dataframe and series\n",
        "\n",
        "x_under_sampled = pd.DataFrame(x_under_sampled, columns = x.columns)\n",
        "y_under_sampled = pd.Series(y_under_sampled, name = 'Class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5i9_4w4uR8n"
      },
      "outputs": [],
      "source": [
        "# Print the value_count of the under_sampled series\n",
        "\n",
        "y_under_sampled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXJU2lSvucBc"
      },
      "outputs": [],
      "source": [
        "# Spilt the data into the training and testing set using train_test_spilt\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_under_sampled, y_under_sampled, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a dictionary of classification models along with their hyperparameters for GridSearchCV during training and evaluation.\n",
        "\n",
        "def get_models():\n",
        "    models = dict()\n",
        "\n",
        "    models['Decision_Tree'] = {\n",
        "        'model': DecisionTreeClassifier(random_state = 42),\n",
        "        'params': {\n",
        "            'max_depth': [3, 5, 10, None],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    models['Logistic_Regression'] = {\n",
        "        'model': LogisticRegression(solver = 'liblinear', max_iter = 1000),\n",
        "        'params': {\n",
        "            'C': [0.01, 0.1, 1, 10],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    models['Random_Forest'] = {\n",
        "        'model': RandomForestClassifier(random_state = 42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100],\n",
        "            'max_depth': [None, 10],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    models['XGBoost'] = {\n",
        "        'model': XGBClassifier(use_label_encoder = False, eval_metric = 'logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'max_depth': [3, 5]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    base_learners = [\n",
        "        ('Random_Forest', RandomForestClassifier(n_estimators = 100, random_state = 42)),\n",
        "        ('XGBoost', XGBClassifier(use_label_encoder = False, eval_metric = 'logloss')),\n",
        "    ]\n",
        "\n",
        "    final_estimator = LogisticRegression(max_iter = 1000)\n",
        "\n",
        "    models['Stacked_Model'] = {\n",
        "\n",
        "        'model': StackingClassifier(estimators = base_learners, final_estimator = final_estimator, cv = 5, n_jobs = -1),\n",
        "        'params': {} # No hyperparameter tuning applied to stacking models\n",
        "    }\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "Cgo3oELnpXYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train, evaluate and save the traditonal machine learning models\n",
        "\n",
        "def evaluate_models(x_train, y_train, x_test, y_test, save_dir ='saved_models'):\n",
        "    models = get_models()\n",
        "    results =[]\n",
        "\n",
        "    for name, mp in models.items():\n",
        "        print(f'\\nModel Name: {name}')\n",
        "\n",
        "        if mp['params']:\n",
        "            grid = GridSearchCV(mp['model'], mp['params'], scoring = 'roc_auc', cv = 5, n_jobs = -1)\n",
        "            grid.fit(x_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            print('Best Parameters:', grid.best_params_)\n",
        "\n",
        "        else:\n",
        "            best_model = mp['model'].fit(x_train, y_train)\n",
        "            best_params = None\n",
        "            print('No hyperparameter tuning applied.')\n",
        "\n",
        "        y_pred = best_model.predict(x_test)\n",
        "        y_probs = best_model.predict_proba(x_test)[:, 1] if hasattr(best_model, \"predict_proba\") else y_pred\n",
        "\n",
        "        # Classification Report\n",
        "        print('\\n Classification Report:')\n",
        "        print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Fraud'], yticklabels = ['Normal', 'Fraud'])\n",
        "        plt.title(f'{name} - Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.show()\n",
        "\n",
        "        # ROC Curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "        auc_score = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        plt.plot(fpr, tpr, label = f'AUC = {auc_score:.2f}')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{name} - ROC Curve')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show()\n",
        "\n",
        "        print(f'AUC Score: {auc_score:.2f}')\n",
        "\n",
        "        # Precision-Recall Curve and Average Precision\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "        avg_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "        # Plot the Precision-Recall Curve\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title(f'{name} - Precision-Recall Curve')\n",
        "        plt.legend(loc='lower left')\n",
        "        plt.show()\n",
        "\n",
        "        print(f'Average Precision (PR-AUC): {avg_precision:.2f}')"
      ],
      "metadata": {
        "id": "NdDNWABQpqRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train, evaluate and save isolation forest model\n",
        "\n",
        "def isolation_forest_model(x_train, x_test, y_test, save_dir = 'saved_models'):\n",
        "    model = IsolationForest(random_state = 42)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(x_train)\n",
        "\n",
        "    # Predict the anomalies\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # Convert anomalies to 1 (fraud), normal to 0\n",
        "    y_pred = np.where(y_pred == -1, 1, 0)\n",
        "\n",
        "    labels = ['Normal', 'Fraud']\n",
        "\n",
        "    # Printing the classification report\n",
        "    print('Model: Isolation_Forest')\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred, target_names = labels))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    cnn_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize = (10, 8))\n",
        "    sns.heatmap(cnn_cm, annot = True, fmt = \"d\", cmap = \"Blues\", cbar = True,\n",
        "                xticklabels = labels, yticklabels = labels)\n",
        "    plt.title(f'Isolation_Forest Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "    # Get the anomaly scores from the Isolation Forest model\n",
        "    y_scores = -model.decision_function(x_test)\n",
        "\n",
        "    # Compute the AUC score\n",
        "    roc_auc = roc_auc_score(y_test, y_scores)\n",
        "\n",
        "    # Calculate the ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label = f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve for Isolation Forest')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall Curve and Average Precision\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "    avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "    # Plot the Precision-Recall Curve\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Average Precision (PR-AUC): {avg_precision:.2f}')"
      ],
      "metadata": {
        "id": "ChKF2U5LOY6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train, evaluate and save an MLP model\n",
        "\n",
        "def build_mlp(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    # Clears the background session before training a new model\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Input shape for the MLP model\n",
        "\n",
        "    input_shape = (x_train.shape[1],)\n",
        "\n",
        "    model = Sequential([\n",
        "\n",
        "        Dense(64, kernel_regularizer=l2(0.001), activation = 'relu', input_shape = input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation = 'relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation = 'relu'),\n",
        "        Dense(1, activation = 'sigmoid')\n",
        "\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer = Adam(1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "    history = model.fit(\n",
        "\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs = 30,\n",
        "        batch_size = 256,\n",
        "        validation_split = 0.1,\n",
        "        verbose = 2\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(x_test).flatten()\n",
        "    y_preds = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Printing the classification report\n",
        "    print(classification_report(y_test, y_preds, target_names = ['Normal','Fraud']))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap ='Blues', xticklabels=['Normal','Fraud'], yticklabels=['Normal','Fraud'])\n",
        "    plt.title('MLP Classifier - Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Compute the AUC score\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate the ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, label = f'ROC-AUC = {roc_auc:.2f}')\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall Curve and Average Precision\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "    ap = average_precision_score(y_test, y_pred)\n",
        "\n",
        "    # Plot the Precision-Recall Curve\n",
        "    plt.plot(recall, precision, label=f'AP={ap:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "YBfVZsqPY3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEgp0poi42Zl"
      },
      "outputs": [],
      "source": [
        "# Apply the function evaluate_models to the under_sampled data\n",
        "evaluate_models(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function isolation_forest_model to the under_sampled data\n",
        "isolation_forest_model(x_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "pBRBNCb6mSgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function build_mlp to the under_sampled data\n",
        "mlp_model, mlp_history = build_mlp(x_train, y_train, x_test, y_test)\n",
        "mlp_model, mlp_history"
      ],
      "metadata": {
        "id": "YzqDCunNrIQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmy4vMOw-XCH"
      },
      "source": [
        "<h3 style=\"text-align: center;\">Random Over Sampling</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkPjobaq-WWX"
      },
      "outputs": [],
      "source": [
        "# Initialize RandomOverSampler\n",
        "ROS = RandomOverSampler(random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Put5ktmi5Hli"
      },
      "outputs": [],
      "source": [
        "# Apply RandomOverSampler\n",
        "x_over_sampled, y_over_sampled = ROS.fit_resample(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhKBhO8VAXGo"
      },
      "outputs": [],
      "source": [
        "# Convert the resampled arrays back to a dataframe and series\n",
        "x_over_sampled = pd.DataFrame(x_over_sampled, columns = x.columns)\n",
        "y_over_sampled = pd.Series(y_over_sampled, name = 'Class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faG5DkD4AeVB"
      },
      "outputs": [],
      "source": [
        "# Print the value_count of the over_sampled series\n",
        "y_over_sampled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EzhBj_oAhpm"
      },
      "outputs": [],
      "source": [
        "# Spilt the data into the training and testing set using train_test_spilt\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_over_sampled, y_over_sampled, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function evaluate_models to the over_sampled data\n",
        "evaluate_models(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "5r8MOSoRT65c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function isolation_forest_model to the over_sampled data\n",
        "isolation_forest_model(x_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "f93JKUfWmYRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function build_mlp to the over_sampled data\n",
        "mlp_model, mlp_history = build_mlp(x_train, y_train, x_test, y_test)\n",
        "mlp_model, mlp_history"
      ],
      "metadata": {
        "id": "zaN16cmuDkTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6chqBDBqBM5w"
      },
      "source": [
        "<h3 style=\"text-align: center;\">SMOTE (Synthetic Minority Over Sampling)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIB_K0JFBZJ7"
      },
      "outputs": [],
      "source": [
        "# Initialize SMOTE\n",
        "Smote = SMOTE(random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KYXNX62BuH1"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE\n",
        "x_smote, y_smote = Smote.fit_resample(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the resampled arrays back to a dataframe and series\n",
        "x_smote = pd.DataFrame(x_smote, columns = x.columns)\n",
        "y_smote = pd.Series(y_smote, name = 'Class')"
      ],
      "metadata": {
        "id": "jKFDoWQnPmbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxgc4csvBzyG"
      },
      "outputs": [],
      "source": [
        "# Print the value count the smote series\n",
        "y_smote.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcXS-abXB8h3"
      },
      "outputs": [],
      "source": [
        "# Spilt the data into the training and testing set using train_test_spilt\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1iIrPa6CA2a"
      },
      "outputs": [],
      "source": [
        "# Apply the function evaluate_models to the smote data\n",
        "evaluate_models(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function isolation_forest_model to the smote data\n",
        "isolation_forest_model(x_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "3cwohTEAmar6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function build_mlp to the smote data\n",
        "mlp_model, mlp_history = build_mlp(x_train, y_train, x_test, y_test)\n",
        "mlp_model, mlp_history"
      ],
      "metadata": {
        "id": "6UzzMAoKVrsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NP_Bzbi58lIqcUddYqUENJc_YklRyQZS",
      "authorship_tag": "ABX9TyMAUFsF9nHmbVF1KjuT6VJF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}